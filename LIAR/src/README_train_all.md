# ğŸ“˜ Training Script Overview â€” `train_all.py`

This script automates the training of **multiple DoubleInput Transformer models** (DistilBERT, BERT, RoBERTa, DeBERTa) on the LIAR-PLUS dataset by calling the `train.py` script programmatically using Python's `subprocess` module.

---

## ğŸ”„ Purpose
- Streamline **batch training** of all model variants in one run.
- Allow user-specific configuration for each model (epochs, class weighting).

---

## ğŸ”§ How It Works

### 1. Model Definitions
- Four models supported:
  - `1` â†’ `distilbert-base-uncased`
  - `2` â†’ `bert-base-uncased`
  - `3` â†’ `roberta-base`
  - `4` â†’ `microsoft/deberta-v3-base`

### 2. User Interaction
- For each model, the script prompts:
  - Number of training epochs
  - Whether to use **class weights** for handling class imbalance (`--weighted`)

### 3. Automated Training
- Constructs the inputs to mimic interactive mode of `train.py`.
- Launches subprocesses to execute training one model at a time:
  ```bash
  python src/train.py --weighted
  ```
  - Input: `<model_number>\n<epochs>\n`

---

## ğŸ“… Output

For each model, `train.py` will create:
- Trained model folder: `models/<model_name>/model/`
- Checkpoints: `models/<model_name>/checkpoints/`
- Log files: `models/<model_name>/logs/train.log`

---

## âš ï¸ Notes
- Script uses `subprocess.run()` with `input` to simulate CLI inputs.
- Will **run sequentially**, one model at a time.
- Errors are caught and reported per model.
- To stop execution midway, use keyboard interrupt (Ctrl+C).

---

## ğŸ“ File Structure

```
project-root/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ train.py
â”œâ”€â”€ train_all.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ train_plus.csv
â”‚   â”‚   â””â”€â”€ valid_plus.csv
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ distilbert-base-uncased/
â”‚   â”œâ”€â”€ bert-base-uncased/
â”‚   â”œâ”€â”€ roberta-base/
â”‚   â””â”€â”€ microsoft/
```

---

## ğŸ‘ Best Practices
- Run this script **after testing** that `train.py` works for each model.
- Ensure `train.py` has interactive prompts compatible with this script.
- Check CUDA/GPU availability before launching for large-scale training.

---

## ğŸš€ Example

```
$ python train_all.py

ğŸ” Batch training setup for all models...
ğŸ•’ How many epochs to train DistilBERT? (e.g., 10.0): 10
ğŸ“Š Use class weights for DistilBERT? (y/n, default: n): y
...
ğŸš€ Starting training for all selected models...
```

All logs and outputs will be stored under `models/`.

